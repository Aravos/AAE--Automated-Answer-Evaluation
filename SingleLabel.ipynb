{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels = 1,output_attentions = False,output_hidden_states = False)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr = 2e-5, eps = 1e-8)\n",
    "current_directory = current_directory = \"C:\\\\07-Project\\\\Final Project 8th Sem\\\\01-NLP\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training = pd.read_csv(\"Custom-Dataset.csv\", encoding=\"utf-8\")\n",
    "data_training.head()\n",
    "questions = []\n",
    "target_answers = []\n",
    "answers = []\n",
    "labels = []\n",
    "max_len = 0\n",
    "\n",
    "for index,row in data_training.iterrows():\n",
    "    target_answers.append(row[\"reference_answer_en\"].translate(str.maketrans('', '', string.punctuation)).lower())\n",
    "    answers.append(row[\"student_answer_en\"].translate(str.maketrans('', '', string.punctuation)).lower())\n",
    "    labels.append(row[\"Average_Mark\"])\n",
    "\n",
    "for i in range(0,len(answers)):\n",
    "    input_ids = tokenizer.encode(answers[i],target_answers[i], add_special_tokens=True)\n",
    "    max_len = max(max_len, len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(answers,target_answers):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "    for i in range(0,len(answers)):\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                        answers[i],\n",
    "                        target_answers[i],\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = max_len,\n",
    "                        truncation = True,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                   )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        token_type_ids.append(encoded_dict['token_type_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    return input_ids,token_type_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids,token_type_ids, attention_masks = preprocessing_for_bert(answers,target_answers)\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "index = data_training.index.tolist()\n",
    "tensor_index = torch.tensor(index)\n",
    "dataset = TensorDataset(input_ids, attention_masks,token_type_ids, labels,tensor_index)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_data, val_data = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_data,os.path.join(current_directory, \"dataset\", \"train_dataset.pt\"))\n",
    "torch.save(val_data,os.path.join(current_directory, \"dataset\", \"test_dataset.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load(os.path.join(current_directory, \"dataset\", \"train_dataset.pt\"))\n",
    "val_data = torch.load(os.path.join(current_directory, \"dataset\", \"test_dataset.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_data,sampler = RandomSampler(train_data),batch_size = batch_size)\n",
    "validation_dataloader = DataLoader(val_data,sampler = SequentialSampler(val_data),batch_size = batch_size)\n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0,num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = preds.flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return pearsonr(pred_flat,labels_flat)[0]\n",
    "def flat_rmse(preds, labels):\n",
    "    pred_flat = preds.flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return sqrt(mean_squared_error(pred_flat,labels_flat))\n",
    "def weights_init(m):\n",
    "  torch.nn.init.xavier_uniform(m.weight.data)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global predictions, indexes, labels_test\n",
    "predictions = np.array([])\n",
    "indexes = np.array([])\n",
    "labels_test = np.array([])\n",
    "def get_predictions(p,l,i):\n",
    "  global predictions, indexes, labels_test\n",
    "  predictions = np.concatenate((predictions,p.flatten()),axis=0)\n",
    "  indexes = np.concatenate((indexes,i.flatten()),axis=0)\n",
    "  labels_test = np.concatenate((labels_test,l.flatten()), axis=0)\n",
    "def flat_accuracy_classification_task(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 10 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('Batch: '+step+\" Time: \"+elapsed)\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_type_token_ids = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "        b_index = batch[4].to(device)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids,token_type_ids=b_type_token_ids,attention_mask=b_input_mask,labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\\nAverage training loss: \",avg_train_loss)\n",
    "    print(\"Training epoch took: \",training_time)\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_type_token_ids = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "        b_index = batch[4]\n",
    "        with torch.no_grad():\n",
    "            output = model(b_input_ids, token_type_ids=b_type_token_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        total_eval_loss += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        get_predictions(logits,label_ids,b_index.numpy())\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    print(\"  Validation Loss: \",round(avg_val_loss,2))\n",
    "    \n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"Total Time \",format_time(time.time()-total_t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.tolist()[-378:]\n",
    "labels_test.tolist()[-378:]\n",
    "indexes.tolist()[-378:]\n",
    "test_data = data_training.iloc[indexes[-378:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantizer(pred):\n",
    "    output = []\n",
    "    quantization_levels = {\n",
    "        5.0: 5.0,\n",
    "        4.5: 4.5,\n",
    "        4.0: 4.0,\n",
    "        3.5: 3.5,\n",
    "        3.0: 3.0,\n",
    "        2.5: 2.5,\n",
    "        2.0: 2.0,\n",
    "        1.5: 1.5,\n",
    "        1.0: 1.0,\n",
    "        0.5: 0.5,\n",
    "        0.0: 0.0\n",
    "    }\n",
    "    for sample in pred:\n",
    "        quantized_value = min(quantization_levels.keys(), key=lambda x: abs(x - sample))\n",
    "        output.append(quantized_value)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"predicted\"] = predictions[-378:]\n",
    "test_data[\"test_label\"] = labels_test[-378:]\n",
    "test_data[\"quantized_prediction\"] = quantizer(predictions[-378:])\n",
    "examples = test_data.sample(5)\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "n = np.arange(len(predictions[-378:]))\n",
    "plt.figure()\n",
    "plt.plot(n,labels_test[-378:],'o',label='Human Score')\n",
    "plt.plot(n,predictions[-378:],'o',label='Score Predicted')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Test Samples')\n",
    "plt.ylabel('Score Answer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax1 = sns.distplot(quantizer(predictions[-378:]), kde=False)\n",
    "ax1.set(xlabel='Score', ylabel='Number of answers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predictions[-378:]\n",
    "fix_pred = np.where(y_pred>5.0,5.0,y_pred)\n",
    "fix_pred = np.where(fix_pred<0.0,0.0,fix_pred)\n",
    "plt.figure()\n",
    "plt.plot(fix_pred,labels_test[-378:],'o') \n",
    "plt.xlabel('Predicted Score')\n",
    "plt.ylabel('Actual Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax1 = sns.distplot(labels_test[-378:], kde=False)\n",
    "ax1.set(xlabel='Score', ylabel='Number of answers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(current_directory, \"BERT-predictions\",\"predictions.npy\"),predictions[-378:])\n",
    "np.save(os.path.join(current_directory, \"BERT-predictions\",\"labels_test.npy\"),labels_test[-378:])\n",
    "np.save(os.path.join(current_directory, \"BERT-predictions\",\"indexes.npy\"),indexes[-378:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style='darkgrid')\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(current_directory, \"BERT_Model\",\"BERT_Model.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
